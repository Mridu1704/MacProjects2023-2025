{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H8-pmIP71Dbc"
   },
   "source": [
    "Translator for German to English"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WXzVbz7420K_"
   },
   "source": [
    "Import the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "yu1gdHn1UfbA"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CRy9tP9i24Vk"
   },
   "source": [
    "Download the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "r9Z6OU2uyQ7i",
    "outputId": "30bb3891-d371-4bac-fbd6-a09bf00e2e09"
   },
   "outputs": [],
   "source": [
    "#from google.colab import drive\n",
    "#drive.mount('/content/drive')\n",
    "\n",
    "file_path = './deu.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IiXKMMTV25-o"
   },
   "source": [
    "Create list of input and target words/sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X_2oXTPUUxHY",
    "outputId": "48c7960e-8e04-4439-e726-8a1302beb532"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ohne Zweifel findet sich auf dieser Welt zu jedem Mann genau die richtige Ehefrau und umgekehrt; wenn man jedoch in Betracht zieht, dass ein Mensch nur Gelegenheit hat, mit ein paar hundert anderen bekannt zu sein, von denen ihm nur ein Dutzend oder weniger nahesteht, darunter höchstens ein oder zwei Freunde, dann erahnt man eingedenk der Millionen Einwohner dieser Welt leicht, dass seit Erschaffung ebenderselben wohl noch nie der richtige Mann der richtigen Frau begegnet ist.\n",
      "Doubtless there exists in this world precisely the right woman for any given man to marry and vice versa; but when you consider that a human being has the opportunity of being acquainted with only a few hundred people, and out of the few hundred that there are but a dozen or less whom he knows intimately, and out of the dozen, one or two friends at most, it will easily be seen, when we remember the number of millions who inhabit this world, that probably, since the earth was created, the right man has never yet met the right woman.\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        reader = csv.reader(file, delimiter='\\t')\n",
    "        pairs = list(reader)\n",
    "\n",
    "context = np.array([pair[1] for pair in pairs])\n",
    "target = np.array([pair[0] for pair in pairs])\n",
    "\n",
    "print(context[-1])\n",
    "print(target[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6lTThbY83CMo"
   },
   "source": [
    "Shuffle the dataset and Create batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "6BDW6daNVgTZ"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "\n",
    "# Generate a boolean array 'is_train' to determine if each sample should be included in the training set.\n",
    "# Each element of 'is_train' is True with a probability of 0.8, indicating inclusion in the training set.\n",
    "is_train = np.random.uniform(size=(len(target),)) < 0.8\n",
    "\n",
    "# Create a training dataset ('train_ten') using samples where 'is_train' is True.\n",
    "# The dataset is constructed from tensor slices of 'context' and 'target' arrays corresponding to True values in 'is_train'.\n",
    "# Shuffle the dataset with a buffer size of the length of 'context' and batch it with the specified BATCH_SIZE.\n",
    "train_ten = (\n",
    "    tf.data.Dataset\n",
    "    .from_tensor_slices((context[is_train], target[is_train]))\n",
    "    .shuffle(len(context))\n",
    "    .batch(BATCH_SIZE))\n",
    "\n",
    "# Create a test dataset ('test_ten') using samples where 'is_train' is False.\n",
    "# The dataset is constructed from tensor slices of 'context' and 'target' arrays corresponding to False values in 'is_train'.\n",
    "# Shuffle the dataset with a buffer size of the length of 'context' and batch it with the specified BATCH_SIZE.\n",
    "test_ten = (\n",
    "    tf.data.Dataset\n",
    "    .from_tensor_slices((context[~is_train], target[~is_train]))\n",
    "    .shuffle(len(context))\n",
    "    .batch(BATCH_SIZE))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f1-e2Lzx3HMa"
   },
   "source": [
    "Preprocess the datatset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "QZ8td485V6FQ"
   },
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "\n",
    "# clean the data by converting to lower case, removing unwanted characters and addinf start and end token\n",
    "def clean_data(text):\n",
    "    # Convert the text to lowercase\n",
    "    text = tf.strings.lower(text)\n",
    "    # Keep only alphabets, digits, and punctuation\n",
    "    text = tf.strings.regex_replace(text, '[^a-z0-9?.!,¿]', ' ')\n",
    "    # Strip leading and trailing whitespaces\n",
    "    text = tf.strings.strip(text)\n",
    "    # Add start and end tokens to the text\n",
    "    text = tf.strings.join(['[START]', text, '[END]'], separator=' ')\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lzkSrW9fc1V5"
   },
   "source": [
    "Tokenize and Vectorize the text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WNZSH9hWV-yw",
    "outputId": "41b73e46-8da2-4961-b245-861193eb88af"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context Vocabulary: ['', '[UNK]', '[START]', '[END]', 'ich', 'tom', 'ist', 'nicht', 'das', 'du']\n",
      "Target Vocabulary: ['', '[UNK]', '[START]', '[END]', 'i', 'tom', 'to', 'the', 'you', 'a']\n"
     ]
    }
   ],
   "source": [
    "max_vocab_size = 5000  # Define the maximum vocabulary size\n",
    "\n",
    "# Define the text vectorization layer for context\n",
    "context_vectorization = tf.keras.layers.TextVectorization(\n",
    "    standardize=clean_data,  # Preprocessing function for standardization\n",
    "    max_tokens=max_vocab_size,  # Maximum vocabulary size\n",
    "    ragged=True  # Allow ragged tensors (varying-length sequences)\n",
    ")\n",
    "# Adapt the text vectorization layer to the training data\n",
    "context_vectorization.adapt(train_ten.map(lambda context, target: context))\n",
    "\n",
    "# Display the first 10 words from the vocabulary\n",
    "context_vocab = context_vectorization.get_vocabulary()[:10]\n",
    "print(\"Context Vocabulary:\", context_vocab)\n",
    "\n",
    "# Define the text vectorization layer for target\n",
    "target_vectorization = tf.keras.layers.TextVectorization(\n",
    "    standardize=clean_data,  # Preprocessing function for standardization\n",
    "    max_tokens=max_vocab_size,  # Maximum vocabulary size\n",
    "    ragged=True  # Allow ragged tensors (varying-length sequences)\n",
    ")\n",
    "# Adapt the text vectorization layer to the training data\n",
    "target_vectorization.adapt(train_ten.map(lambda context, target: target))\n",
    "\n",
    "# Display the first 10 words from the vocabulary\n",
    "target_vocab = target_vectorization.get_vocabulary()[:10]\n",
    "print(\"Target Vocabulary:\", target_vocab)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "vCMHZoHUWDn4"
   },
   "outputs": [],
   "source": [
    "def process_text(context, target):\n",
    "    # Process context and target texts using the TextVectorization layers\n",
    "    context = context_vectorization(context)\n",
    "    target = target_vectorization(target)\n",
    "\n",
    "    # Get inputs and outputs for the target text (teacher forcing)\n",
    "    targ_in = target[:, :-1]\n",
    "    targ_out = target[:, 1:]\n",
    "\n",
    "    # Return processed data\n",
    "    return (context.to_tensor(), targ_in.to_tensor()), targ_out.to_tensor()\n",
    "\n",
    "# Map the process_text function to create training and validation datasets\n",
    "train_ds = train_ten.map(process_text, tf.data.AUTOTUNE)\n",
    "test_ds = test_ten.map(process_text, tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GjqFo1I63NkO"
   },
   "source": [
    "Encoder using LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "yQqO1zgoWXQr"
   },
   "outputs": [],
   "source": [
    "rnn_units = 128\n",
    "class Encoder(tf.keras.layers.Layer):\n",
    "  def __init__(self, text_processor, units):\n",
    "    super(Encoder, self).__init__()\n",
    "    self.text_processor = text_processor\n",
    "    self.vocab_size = text_processor.vocabulary_size()\n",
    "    self.units = units\n",
    "    # converts tokens to vectors\n",
    "    self.embedding = tf.keras.layers.Embedding(self.vocab_size, units,\n",
    "                                               mask_zero=True)\n",
    "    # The RNN layer processes those vectors sequentially.\n",
    "    # Initialize a Bidirectional LSTM layer\n",
    "    self.rnn = tf.keras.layers.Bidirectional(\n",
    "      # Parameter for specifying how the outputs of the forward and backward LSTMs are merged\n",
    "      merge_mode='sum',\n",
    "      # Parameter specifying the recurrent layer used for bidirectional processing\n",
    "      layer=tf.keras.layers.LSTM(\n",
    "        units,  # Dimensionality of the output space (number of units in LSTM cell)\n",
    "        return_sequences=True, # Return the full sequence of outputs for each timestep\n",
    "        recurrent_initializer='glorot_uniform'  # Initializer for the recurrent weights\n",
    "    )\n",
    ")\n",
    "  def call(self, x):\n",
    "    x = self.embedding(x)\n",
    "    x = self.rnn(x)\n",
    "    return x\n",
    "\n",
    "  def input_to_tensor(self, texts):\n",
    "    texts = tf.convert_to_tensor(texts)\n",
    "    if len(texts.shape) == 0:\n",
    "      texts = tf.convert_to_tensor(texts)[tf.newaxis]\n",
    "    context = self.text_processor(texts).to_tensor()\n",
    "    context = self(context)\n",
    "    return context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EKfHOU3h3O9D"
   },
   "source": [
    "Attention Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "V86x2MOeWm4G"
   },
   "outputs": [],
   "source": [
    "class Attention_Layer(tf.keras.layers.Layer):\n",
    "    def __init__(self, units, **kwargs):\n",
    "        super().__init__()\n",
    "        # Initialize a MultiHeadAttention layer\n",
    "        self.mha = tf.keras.layers.MultiHeadAttention(key_dim=units, num_heads=1, **kwargs)\n",
    "        # Initialize a LayerNormalization layer\n",
    "        self.layernorm = tf.keras.layers.LayerNormalization()\n",
    "        # Initialize an Add layer\n",
    "        self.add = tf.keras.layers.Add()\n",
    "\n",
    "    def call(self, x, context):\n",
    "        # Perform MultiHeadAttention\n",
    "        attn_output, attn_scores = self.mha(\n",
    "            query=x,\n",
    "            value=context,\n",
    "            return_attention_scores=True)\n",
    "        # Calculate mean attention scores across heads\n",
    "        attn_scores = tf.reduce_mean(attn_scores, axis=1)\n",
    "        # Add the attention output to the input\n",
    "        x = self.add([x, attn_output])\n",
    "        # Apply layer normalization\n",
    "        x = self.layernorm(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QVO6cZI53Sx6"
   },
   "source": [
    "Decoder Layer using LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "PKxHHwZXWz9T"
   },
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, text_processor, units):\n",
    "        super(Decoder, self).__init__()\n",
    "        # Initialize the text processor\n",
    "        self.text_processor = text_processor\n",
    "        # Get vocabulary size\n",
    "        self.vocab_size = text_processor.vocabulary_size()\n",
    "        # Initialize StringLookup layers for token conversion\n",
    "        self.char_to_id = tf.keras.layers.StringLookup(\n",
    "            vocabulary=text_processor.get_vocabulary(),\n",
    "            mask_token='', oov_token='[UNK]')\n",
    "        # convert the numerical ids back to text\n",
    "        self.id_to_char = tf.keras.layers.StringLookup(\n",
    "            vocabulary=text_processor.get_vocabulary(),\n",
    "            mask_token='', oov_token='[UNK]',\n",
    "            invert=True)\n",
    "\n",
    "        # Define start and end tokens\n",
    "        self.start_token = self.char_to_id('[START]')\n",
    "        self.end_token = self.char_to_id('[END]')\n",
    "        # Define the number of units\n",
    "        self.units = units\n",
    "        # 1. The embedding layer converts token IDs to vectors\n",
    "        self.embedding = tf.keras.layers.Embedding(self.vocab_size,\n",
    "                                                   units, mask_zero=True)\n",
    "        # 2. The RNN keeps track of what's been generated so far.\n",
    "        self.rnn = tf.keras.layers.LSTM(units,\n",
    "                                         return_sequences=True,\n",
    "                                         return_state=True,\n",
    "                                         recurrent_initializer='glorot_uniform')\n",
    "        # 3. The RNN output will be the query for the attention layer.\n",
    "        self.attention = Attention_Layer(units)\n",
    "        # 4. This fully connected layer produces the logits for each\n",
    "        # output token.\n",
    "        self.output_layer = tf.keras.layers.Dense(self.vocab_size)\n",
    "\n",
    "    def call(self, context, x, state=None, return_state=False):\n",
    "        # 1. Lookup the embeddings\n",
    "        x = self.embedding(x)\n",
    "        # 2. Process the target sequence.\n",
    "        state = self.rnn.get_initial_state(x)\n",
    "        x, *state = self.rnn(x, initial_state=state)\n",
    "        # 3. Use the RNN output as the query for the attention over the context.\n",
    "        x = self.attention(x, context)\n",
    "        # Step 4. Generate logit predictions for the next token.\n",
    "        logits = self.output_layer(x)\n",
    "\n",
    "        if return_state:\n",
    "            return logits, state\n",
    "        else:\n",
    "            return logits\n",
    "\n",
    "    def get_initial_state(self, context):\n",
    "        batch_size = tf.shape(context)[0]\n",
    "        start_tokens = tf.fill([batch_size, 1], self.start_token)\n",
    "        done = tf.zeros([batch_size, 1], dtype=tf.bool)\n",
    "        embedded = self.embedding(start_tokens)\n",
    "        return start_tokens, done, self.rnn.get_initial_state(embedded)[0]\n",
    "\n",
    "    def tokens_to_text(self, tokens):\n",
    "        words = self.id_to_char(tokens)\n",
    "        result = tf.strings.reduce_join(words, axis=-1, separator=' ')\n",
    "        result = tf.strings.regex_replace(result, '^ *\\[START\\] *', '')\n",
    "        result = tf.strings.regex_replace(result, ' *\\[END\\] *$', '')\n",
    "        return result\n",
    "\n",
    "    def get_next_token(self, context, next_token, done, state):\n",
    "        logits, state = self(\n",
    "            context, next_token,\n",
    "            state=state,\n",
    "            return_state=True)\n",
    "\n",
    "        next_token = tf.argmax(logits, axis=-1)\n",
    "        # If a sequence produces an `end_token`, set it `done`\n",
    "        done = done | (next_token == self.end_token)\n",
    "        # Once a sequence is done it only produces 0-padding.\n",
    "        next_token = tf.where(done, tf.constant(0, dtype=tf.int64), next_token)\n",
    "\n",
    "        return next_token, done, state\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z8jlQuR33pXJ"
   },
   "source": [
    "Translator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "mHcLLy04XN3U"
   },
   "outputs": [],
   "source": [
    "class Translator(tf.keras.Model):\n",
    "  def __init__(self, units,\n",
    "               context_vectorization,\n",
    "               target_vectorization):\n",
    "    super().__init__()\n",
    "    # Build the encoder and decoder\n",
    "    encoder = Encoder(context_vectorization, units)\n",
    "    decoder = Decoder(target_vectorization, units)\n",
    "\n",
    "    self.encoder = encoder\n",
    "    self.decoder = decoder\n",
    "\n",
    "  def call(self, inputs):\n",
    "    context, x = inputs\n",
    "    context = self.encoder(context)\n",
    "    logits = self.decoder(context, x)\n",
    "    return logits\n",
    "\n",
    "  def translate(self,\n",
    "                texts, *,\n",
    "                max_length=50):\n",
    "    # Process the input texts\n",
    "    context = self.encoder.input_to_tensor(texts)\n",
    "    batch_size = tf.shape(texts)[0]\n",
    "\n",
    "    # Setup the loop inputs\n",
    "    tokens = []\n",
    "    next_token, done, state = self.decoder.get_initial_state(context)\n",
    "\n",
    "    for _ in range(max_length):\n",
    "      # Generate the next token\n",
    "      next_token, done, state = self.decoder.get_next_token(\n",
    "          context, next_token, done,  state)\n",
    "\n",
    "      # Collect the generated tokens\n",
    "      tokens.append(next_token)\n",
    "\n",
    "      if tf.executing_eagerly() and tf.reduce_all(done):\n",
    "        break\n",
    "\n",
    "    # Stack the lists of tokens and attention weights.\n",
    "    tokens = tf.concat(tokens, axis=-1)   # t*[(batch 1)] -> (batch, t)\n",
    "\n",
    "    result = self.decoder.tokens_to_text(tokens)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "sBP_tN1vXdpJ"
   },
   "outputs": [],
   "source": [
    "model = Translator(rnn_units, context_vectorization, target_vectorization)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "ZkEVO-jOXjiA"
   },
   "outputs": [],
   "source": [
    "def fn_loss(y_true, y_pred):\n",
    "    loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "        from_logits=True, reduction='none')\n",
    "    loss = loss_fn(y_true, y_pred)\n",
    "    return tf.reduce_mean(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "PlASMxnBXmKi"
   },
   "outputs": [],
   "source": [
    "def fn_acc(y_true, y_pred):\n",
    "    y_pred = tf.argmax(y_pred, axis=-1)\n",
    "    y_pred = tf.cast(y_pred, y_true.dtype)\n",
    "    match = tf.cast(y_true == y_pred, tf.float32)\n",
    "    total_tokens = tf.cast(tf.size(y_true), tf.float32)\n",
    "    return tf.reduce_sum(match) / total_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "LU-3yAReXqc_"
   },
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.01)\n",
    "model.compile(optimizer=optimizer,\n",
    "              loss=fn_loss,\n",
    "              metrics=[fn_acc, fn_loss], run_eagerly=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OO78MRNPXueZ",
    "outputId": "76f23e8f-119f-4413-bc17-21ad65454889"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20/20 [==============================] - 7s 92ms/step - loss: 3.9006 - fn_acc: 1.5414e-04 - fn_loss: 8.5165\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'loss': 3.9005684852600098,\n",
       " 'fn_acc': 0.00015413903747685254,\n",
       " 'fn_loss': 8.516472816467285}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(test_ds, steps=20, return_dict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XWqtq9uFXxFn",
    "outputId": "01f19c1e-e093-420d-f8ab-cb2605a31e9f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "20/20 [==============================] - 6s 257ms/step - loss: 1.8080 - fn_acc: 0.5544 - fn_loss: 3.7560 - val_loss: 1.4470 - val_fn_acc: 0.5890 - val_fn_loss: 2.8408\n",
      "Epoch 2/100\n",
      "20/20 [==============================] - 5s 247ms/step - loss: 1.2353 - fn_acc: 0.6166 - fn_loss: 2.5743 - val_loss: 1.2977 - val_fn_acc: 0.6060 - val_fn_loss: 2.5847\n",
      "Epoch 3/100\n",
      "20/20 [==============================] - 5s 242ms/step - loss: 1.2858 - fn_acc: 0.6149 - fn_loss: 2.5470 - val_loss: 1.0638 - val_fn_acc: 0.6526 - val_fn_loss: 2.2883\n",
      "Epoch 4/100\n",
      "20/20 [==============================] - 5s 235ms/step - loss: 1.2058 - fn_acc: 0.6298 - fn_loss: 2.4107 - val_loss: 1.1522 - val_fn_acc: 0.6442 - val_fn_loss: 2.2913\n",
      "Epoch 5/100\n",
      "20/20 [==============================] - 5s 251ms/step - loss: 1.1812 - fn_acc: 0.6426 - fn_loss: 2.3199 - val_loss: 1.0904 - val_fn_acc: 0.6609 - val_fn_loss: 2.1947\n",
      "Epoch 6/100\n",
      "20/20 [==============================] - 5s 228ms/step - loss: 1.0519 - fn_acc: 0.6686 - fn_loss: 2.1296 - val_loss: 0.9249 - val_fn_acc: 0.6921 - val_fn_loss: 1.9664\n",
      "Epoch 7/100\n",
      "20/20 [==============================] - 5s 240ms/step - loss: 0.9528 - fn_acc: 0.6898 - fn_loss: 1.9779 - val_loss: 1.0450 - val_fn_acc: 0.6745 - val_fn_loss: 2.0610\n",
      "Epoch 8/100\n",
      "20/20 [==============================] - 5s 266ms/step - loss: 0.8305 - fn_acc: 0.7091 - fn_loss: 1.8163 - val_loss: 1.0261 - val_fn_acc: 0.6800 - val_fn_loss: 1.9905\n",
      "Epoch 9/100\n",
      "20/20 [==============================] - 5s 264ms/step - loss: 0.8821 - fn_acc: 0.7063 - fn_loss: 1.8435 - val_loss: 0.9782 - val_fn_acc: 0.6909 - val_fn_loss: 1.9307\n",
      "Epoch 10/100\n",
      "20/20 [==============================] - 5s 238ms/step - loss: 0.9124 - fn_acc: 0.7018 - fn_loss: 1.8709 - val_loss: 0.8697 - val_fn_acc: 0.7165 - val_fn_loss: 1.7756\n",
      "Epoch 11/100\n",
      "20/20 [==============================] - 5s 249ms/step - loss: 0.8541 - fn_acc: 0.7177 - fn_loss: 1.7505 - val_loss: 0.8001 - val_fn_acc: 0.7290 - val_fn_loss: 1.6775\n",
      "Epoch 12/100\n",
      "20/20 [==============================] - 6s 301ms/step - loss: 0.8384 - fn_acc: 0.7221 - fn_loss: 1.7271 - val_loss: 0.8385 - val_fn_acc: 0.7198 - val_fn_loss: 1.7130\n",
      "Epoch 13/100\n",
      "20/20 [==============================] - 5s 274ms/step - loss: 0.8335 - fn_acc: 0.7222 - fn_loss: 1.7138 - val_loss: 0.7732 - val_fn_acc: 0.7340 - val_fn_loss: 1.6320\n",
      "Epoch 14/100\n",
      "20/20 [==============================] - 6s 300ms/step - loss: 0.7730 - fn_acc: 0.7360 - fn_loss: 1.6286 - val_loss: 0.7970 - val_fn_acc: 0.7271 - val_fn_loss: 1.6445\n",
      "Epoch 15/100\n",
      "20/20 [==============================] - 6s 318ms/step - loss: 0.7775 - fn_acc: 0.7362 - fn_loss: 1.6082 - val_loss: 0.7911 - val_fn_acc: 0.7391 - val_fn_loss: 1.5787\n",
      "Epoch 16/100\n",
      "20/20 [==============================] - 7s 351ms/step - loss: 0.8054 - fn_acc: 0.7313 - fn_loss: 1.6278 - val_loss: 0.7258 - val_fn_acc: 0.7458 - val_fn_loss: 1.5232\n",
      "Epoch 17/100\n",
      "20/20 [==============================] - 7s 350ms/step - loss: 0.7053 - fn_acc: 0.7509 - fn_loss: 1.5126 - val_loss: 0.7565 - val_fn_acc: 0.7444 - val_fn_loss: 1.5384\n",
      "Epoch 18/100\n",
      "20/20 [==============================] - 7s 338ms/step - loss: 0.8019 - fn_acc: 0.7348 - fn_loss: 1.5961 - val_loss: 0.6471 - val_fn_acc: 0.7670 - val_fn_loss: 1.4129\n",
      "Epoch 19/100\n",
      "20/20 [==============================] - 6s 284ms/step - loss: 0.6762 - fn_acc: 0.7592 - fn_loss: 1.4497 - val_loss: 0.6596 - val_fn_acc: 0.7631 - val_fn_loss: 1.4105\n",
      "Epoch 20/100\n",
      "20/20 [==============================] - 5s 254ms/step - loss: 0.6690 - fn_acc: 0.7599 - fn_loss: 1.4408 - val_loss: 0.6488 - val_fn_acc: 0.7629 - val_fn_loss: 1.3999\n",
      "Epoch 21/100\n",
      "20/20 [==============================] - 6s 302ms/step - loss: 0.6743 - fn_acc: 0.7610 - fn_loss: 1.4350 - val_loss: 0.7430 - val_fn_acc: 0.7529 - val_fn_loss: 1.4721\n",
      "Epoch 22/100\n",
      "20/20 [==============================] - 7s 343ms/step - loss: 0.7090 - fn_acc: 0.7564 - fn_loss: 1.4458 - val_loss: 0.6939 - val_fn_acc: 0.7592 - val_fn_loss: 1.4261\n",
      "Epoch 23/100\n",
      "20/20 [==============================] - 7s 326ms/step - loss: 0.6750 - fn_acc: 0.7629 - fn_loss: 1.4127 - val_loss: 0.6296 - val_fn_acc: 0.7735 - val_fn_loss: 1.3370\n",
      "Epoch 24/100\n",
      "20/20 [==============================] - 7s 343ms/step - loss: 0.6101 - fn_acc: 0.7722 - fn_loss: 1.3295 - val_loss: 0.7325 - val_fn_acc: 0.7547 - val_fn_loss: 1.4429\n",
      "Epoch 25/100\n",
      "20/20 [==============================] - 5s 279ms/step - loss: 0.6307 - fn_acc: 0.7702 - fn_loss: 1.3571 - val_loss: 0.6403 - val_fn_acc: 0.7713 - val_fn_loss: 1.3487\n",
      "Epoch 26/100\n",
      "20/20 [==============================] - 5s 274ms/step - loss: 0.7616 - fn_acc: 0.7442 - fn_loss: 1.4766 - val_loss: 0.6601 - val_fn_acc: 0.7667 - val_fn_loss: 1.3563\n",
      "Epoch 27/100\n",
      "20/20 [==============================] - 6s 297ms/step - loss: 0.6443 - fn_acc: 0.7692 - fn_loss: 1.3244 - val_loss: 0.6160 - val_fn_acc: 0.7755 - val_fn_loss: 1.3056\n",
      "Epoch 28/100\n",
      "20/20 [==============================] - 6s 291ms/step - loss: 0.6027 - fn_acc: 0.7737 - fn_loss: 1.3018 - val_loss: 0.5877 - val_fn_acc: 0.7787 - val_fn_loss: 1.2668\n",
      "Epoch 29/100\n",
      "20/20 [==============================] - 6s 295ms/step - loss: 0.7121 - fn_acc: 0.7605 - fn_loss: 1.3909 - val_loss: 0.6388 - val_fn_acc: 0.7715 - val_fn_loss: 1.3121\n",
      "Epoch 30/100\n",
      "20/20 [==============================] - 5s 275ms/step - loss: 0.6451 - fn_acc: 0.7673 - fn_loss: 1.3456 - val_loss: 0.6068 - val_fn_acc: 0.7782 - val_fn_loss: 1.2686\n",
      "Epoch 31/100\n",
      "20/20 [==============================] - 6s 309ms/step - loss: 0.6238 - fn_acc: 0.7755 - fn_loss: 1.3067 - val_loss: 0.6717 - val_fn_acc: 0.7714 - val_fn_loss: 1.3431\n",
      "Epoch 32/100\n",
      "20/20 [==============================] - 6s 298ms/step - loss: 0.6554 - fn_acc: 0.7687 - fn_loss: 1.3285 - val_loss: 0.5414 - val_fn_acc: 0.7908 - val_fn_loss: 1.2047\n",
      "Epoch 33/100\n",
      "20/20 [==============================] - 6s 308ms/step - loss: 0.6914 - fn_acc: 0.7623 - fn_loss: 1.3681 - val_loss: 0.5925 - val_fn_acc: 0.7782 - val_fn_loss: 1.2609\n",
      "Epoch 34/100\n",
      "20/20 [==============================] - 6s 321ms/step - loss: 0.4507 - fn_acc: 0.8123 - fn_loss: 1.0818 - val_loss: 0.5856 - val_fn_acc: 0.7819 - val_fn_loss: 1.2314\n",
      "Epoch 35/100\n",
      "20/20 [==============================] - 7s 330ms/step - loss: 0.6224 - fn_acc: 0.7773 - fn_loss: 1.2710 - val_loss: 0.5871 - val_fn_acc: 0.7849 - val_fn_loss: 1.2194\n",
      "Epoch 36/100\n",
      "20/20 [==============================] - 7s 350ms/step - loss: 0.5650 - fn_acc: 0.7844 - fn_loss: 1.2237 - val_loss: 0.6313 - val_fn_acc: 0.7754 - val_fn_loss: 1.2952\n",
      "Epoch 37/100\n",
      "20/20 [==============================] - 7s 343ms/step - loss: 0.5991 - fn_acc: 0.7811 - fn_loss: 1.2378 - val_loss: 0.5665 - val_fn_acc: 0.7879 - val_fn_loss: 1.1964\n",
      "Epoch 38/100\n",
      "20/20 [==============================] - 7s 337ms/step - loss: 0.6603 - fn_acc: 0.7728 - fn_loss: 1.2832 - val_loss: 0.6093 - val_fn_acc: 0.7767 - val_fn_loss: 1.2554\n",
      "Epoch 39/100\n",
      "20/20 [==============================] - 6s 313ms/step - loss: 0.6058 - fn_acc: 0.7786 - fn_loss: 1.2530 - val_loss: 0.5908 - val_fn_acc: 0.7825 - val_fn_loss: 1.2326\n",
      "Epoch 40/100\n",
      "20/20 [==============================] - 6s 330ms/step - loss: 0.6156 - fn_acc: 0.7788 - fn_loss: 1.2521 - val_loss: 0.6279 - val_fn_acc: 0.7792 - val_fn_loss: 1.2529\n",
      "Epoch 41/100\n",
      "20/20 [==============================] - 6s 306ms/step - loss: 0.6696 - fn_acc: 0.7701 - fn_loss: 1.3089 - val_loss: 0.5918 - val_fn_acc: 0.7835 - val_fn_loss: 1.2162\n",
      "Epoch 42/100\n",
      "20/20 [==============================] - 7s 337ms/step - loss: 0.5311 - fn_acc: 0.7925 - fn_loss: 1.1677 - val_loss: 0.6038 - val_fn_acc: 0.7831 - val_fn_loss: 1.2129\n",
      "Epoch 43/100\n",
      "20/20 [==============================] - 6s 325ms/step - loss: 0.5763 - fn_acc: 0.7867 - fn_loss: 1.1990 - val_loss: 0.5665 - val_fn_acc: 0.7902 - val_fn_loss: 1.1707\n",
      "Epoch 44/100\n",
      "20/20 [==============================] - 6s 329ms/step - loss: 0.6057 - fn_acc: 0.7799 - fn_loss: 1.2281 - val_loss: 0.5883 - val_fn_acc: 0.7920 - val_fn_loss: 1.1868\n",
      "Epoch 45/100\n",
      "20/20 [==============================] - 7s 344ms/step - loss: 0.6042 - fn_acc: 0.7805 - fn_loss: 1.2095 - val_loss: 0.5341 - val_fn_acc: 0.7955 - val_fn_loss: 1.1429\n",
      "Epoch 46/100\n",
      "20/20 [==============================] - 7s 341ms/step - loss: 0.5727 - fn_acc: 0.7893 - fn_loss: 1.1777 - val_loss: 0.6533 - val_fn_acc: 0.7718 - val_fn_loss: 1.2785\n",
      "Epoch 47/100\n",
      "20/20 [==============================] - 7s 333ms/step - loss: 0.5503 - fn_acc: 0.7933 - fn_loss: 1.1425 - val_loss: 0.6567 - val_fn_acc: 0.7728 - val_fn_loss: 1.2695\n",
      "Epoch 48/100\n",
      "20/20 [==============================] - 6s 307ms/step - loss: 0.5608 - fn_acc: 0.7884 - fn_loss: 1.1786 - val_loss: 0.5805 - val_fn_acc: 0.7890 - val_fn_loss: 1.1648\n",
      "Epoch 49/100\n",
      "20/20 [==============================] - 6s 325ms/step - loss: 0.6037 - fn_acc: 0.7853 - fn_loss: 1.2071 - val_loss: 0.5107 - val_fn_acc: 0.8023 - val_fn_loss: 1.0995\n",
      "Epoch 50/100\n",
      "20/20 [==============================] - 6s 310ms/step - loss: 0.4913 - fn_acc: 0.8059 - fn_loss: 1.0625 - val_loss: 0.5460 - val_fn_acc: 0.7990 - val_fn_loss: 1.1094\n",
      "Epoch 51/100\n",
      "20/20 [==============================] - 6s 292ms/step - loss: 0.5701 - fn_acc: 0.7877 - fn_loss: 1.1667 - val_loss: 0.5467 - val_fn_acc: 0.7984 - val_fn_loss: 1.1100\n",
      "Epoch 52/100\n",
      "20/20 [==============================] - 6s 317ms/step - loss: 0.5494 - fn_acc: 0.7941 - fn_loss: 1.1351 - val_loss: 0.5192 - val_fn_acc: 0.8008 - val_fn_loss: 1.0949\n",
      "Epoch 53/100\n",
      "20/20 [==============================] - 7s 350ms/step - loss: 0.4997 - fn_acc: 0.8049 - fn_loss: 1.0764 - val_loss: 0.5086 - val_fn_acc: 0.8062 - val_fn_loss: 1.0730\n",
      "Epoch 54/100\n",
      "20/20 [==============================] - 7s 343ms/step - loss: 0.5023 - fn_acc: 0.8031 - fn_loss: 1.0779 - val_loss: 0.4711 - val_fn_acc: 0.8114 - val_fn_loss: 1.0155\n",
      "Epoch 55/100\n",
      "20/20 [==============================] - 7s 359ms/step - loss: 0.5212 - fn_acc: 0.7995 - fn_loss: 1.0910 - val_loss: 0.5811 - val_fn_acc: 0.7927 - val_fn_loss: 1.1355\n",
      "Epoch 56/100\n",
      "20/20 [==============================] - 7s 382ms/step - loss: 0.5640 - fn_acc: 0.7948 - fn_loss: 1.1265 - val_loss: 0.5599 - val_fn_acc: 0.7942 - val_fn_loss: 1.1409\n",
      "Epoch 57/100\n",
      "20/20 [==============================] - 7s 383ms/step - loss: 0.5770 - fn_acc: 0.7931 - fn_loss: 1.1435 - val_loss: 0.4912 - val_fn_acc: 0.8057 - val_fn_loss: 1.0528\n",
      "Epoch 58/100\n",
      "20/20 [==============================] - 7s 379ms/step - loss: 0.5510 - fn_acc: 0.7947 - fn_loss: 1.1249 - val_loss: 0.5118 - val_fn_acc: 0.8042 - val_fn_loss: 1.0770\n",
      "Epoch 59/100\n",
      "20/20 [==============================] - 7s 367ms/step - loss: 0.5329 - fn_acc: 0.8010 - fn_loss: 1.0895 - val_loss: 0.5543 - val_fn_acc: 0.7950 - val_fn_loss: 1.1133\n",
      "Epoch 60/100\n",
      "20/20 [==============================] - 8s 425ms/step - loss: 0.5481 - fn_acc: 0.7951 - fn_loss: 1.1300 - val_loss: 0.4866 - val_fn_acc: 0.8073 - val_fn_loss: 1.0465\n",
      "Epoch 61/100\n",
      "20/20 [==============================] - 8s 427ms/step - loss: 0.4397 - fn_acc: 0.8192 - fn_loss: 0.9818 - val_loss: 0.5943 - val_fn_acc: 0.7896 - val_fn_loss: 1.1441\n",
      "Epoch 62/100\n",
      "20/20 [==============================] - 8s 401ms/step - loss: 0.4483 - fn_acc: 0.8176 - fn_loss: 0.9862 - val_loss: 0.4540 - val_fn_acc: 0.8147 - val_fn_loss: 0.9995\n",
      "Epoch 63/100\n",
      "20/20 [==============================] - 9s 439ms/step - loss: 0.5239 - fn_acc: 0.8002 - fn_loss: 1.0663 - val_loss: 0.5367 - val_fn_acc: 0.8011 - val_fn_loss: 1.0774\n",
      "Epoch 64/100\n",
      "20/20 [==============================] - 9s 439ms/step - loss: 0.4890 - fn_acc: 0.8077 - fn_loss: 1.0402 - val_loss: 0.4494 - val_fn_acc: 0.8189 - val_fn_loss: 0.9741\n",
      "Epoch 65/100\n",
      "20/20 [==============================] - 9s 445ms/step - loss: 0.5714 - fn_acc: 0.7927 - fn_loss: 1.1197 - val_loss: 0.5126 - val_fn_acc: 0.8117 - val_fn_loss: 1.0374\n",
      "Epoch 66/100\n",
      "20/20 [==============================] - 9s 461ms/step - loss: 0.4826 - fn_acc: 0.8062 - fn_loss: 1.0350 - val_loss: 0.4936 - val_fn_acc: 0.8055 - val_fn_loss: 1.0315\n",
      "Epoch 67/100\n",
      "20/20 [==============================] - 8s 405ms/step - loss: 0.5226 - fn_acc: 0.8046 - fn_loss: 1.0508 - val_loss: 0.5233 - val_fn_acc: 0.8045 - val_fn_loss: 1.0581\n",
      "Epoch 68/100\n",
      "20/20 [==============================] - 9s 455ms/step - loss: 0.4471 - fn_acc: 0.8191 - fn_loss: 0.9713 - val_loss: 0.5372 - val_fn_acc: 0.7960 - val_fn_loss: 1.0892\n",
      "Epoch 69/100\n",
      "20/20 [==============================] - 9s 457ms/step - loss: 0.5168 - fn_acc: 0.7995 - fn_loss: 1.0846 - val_loss: 0.4784 - val_fn_acc: 0.8143 - val_fn_loss: 1.0077\n",
      "Epoch 70/100\n",
      "20/20 [==============================] - 9s 469ms/step - loss: 0.4857 - fn_acc: 0.8105 - fn_loss: 1.0167 - val_loss: 0.4706 - val_fn_acc: 0.8135 - val_fn_loss: 1.0053\n",
      "Epoch 71/100\n",
      "20/20 [==============================] - 9s 447ms/step - loss: 0.4678 - fn_acc: 0.8150 - fn_loss: 0.9959 - val_loss: 0.5273 - val_fn_acc: 0.8001 - val_fn_loss: 1.0587\n",
      "Epoch 72/100\n",
      "20/20 [==============================] - 10s 488ms/step - loss: 0.5139 - fn_acc: 0.8089 - fn_loss: 1.0284 - val_loss: 0.5075 - val_fn_acc: 0.8094 - val_fn_loss: 1.0233\n",
      "Epoch 73/100\n",
      "20/20 [==============================] - 9s 465ms/step - loss: 0.5688 - fn_acc: 0.7958 - fn_loss: 1.1054 - val_loss: 0.5585 - val_fn_acc: 0.7922 - val_fn_loss: 1.0974\n",
      "Epoch 74/100\n",
      "20/20 [==============================] - 9s 461ms/step - loss: 0.5391 - fn_acc: 0.7971 - fn_loss: 1.0973 - val_loss: 0.5416 - val_fn_acc: 0.7994 - val_fn_loss: 1.0774\n",
      "Epoch 75/100\n",
      "20/20 [==============================] - 8s 429ms/step - loss: 0.5349 - fn_acc: 0.8001 - fn_loss: 1.0611 - val_loss: 0.5153 - val_fn_acc: 0.8049 - val_fn_loss: 1.0423\n",
      "Epoch 76/100\n",
      "20/20 [==============================] - 8s 426ms/step - loss: 0.5013 - fn_acc: 0.8067 - fn_loss: 1.0281 - val_loss: 0.4571 - val_fn_acc: 0.8146 - val_fn_loss: 0.9854\n",
      "Epoch 77/100\n",
      "20/20 [==============================] - 9s 465ms/step - loss: 0.5026 - fn_acc: 0.8062 - fn_loss: 1.0326 - val_loss: 0.5171 - val_fn_acc: 0.8039 - val_fn_loss: 1.0439\n",
      "Epoch 78/100\n",
      "20/20 [==============================] - 9s 445ms/step - loss: 0.5205 - fn_acc: 0.8079 - fn_loss: 1.0272 - val_loss: 0.4610 - val_fn_acc: 0.8155 - val_fn_loss: 0.9853\n",
      "Epoch 79/100\n",
      "20/20 [==============================] - 9s 476ms/step - loss: 0.4610 - fn_acc: 0.8145 - fn_loss: 0.9795 - val_loss: 0.4729 - val_fn_acc: 0.8130 - val_fn_loss: 0.9955\n",
      "Epoch 80/100\n",
      "20/20 [==============================] - 8s 405ms/step - loss: 0.5161 - fn_acc: 0.7994 - fn_loss: 1.0547 - val_loss: 0.5022 - val_fn_acc: 0.8095 - val_fn_loss: 1.0177\n",
      "Epoch 81/100\n",
      "20/20 [==============================] - 9s 459ms/step - loss: 0.4794 - fn_acc: 0.8146 - fn_loss: 0.9860 - val_loss: 0.5242 - val_fn_acc: 0.8044 - val_fn_loss: 1.0405\n",
      "Epoch 82/100\n",
      "20/20 [==============================] - 8s 424ms/step - loss: 0.4942 - fn_acc: 0.8095 - fn_loss: 1.0295 - val_loss: 0.4409 - val_fn_acc: 0.8217 - val_fn_loss: 0.9466\n",
      "Epoch 83/100\n",
      "20/20 [==============================] - 9s 465ms/step - loss: 0.4601 - fn_acc: 0.8167 - fn_loss: 0.9744 - val_loss: 0.5124 - val_fn_acc: 0.8068 - val_fn_loss: 1.0242\n",
      "Epoch 84/100\n",
      "20/20 [==============================] - 9s 465ms/step - loss: 0.4504 - fn_acc: 0.8188 - fn_loss: 0.9653 - val_loss: 0.5037 - val_fn_acc: 0.8084 - val_fn_loss: 0.9955\n",
      "Epoch 85/100\n",
      "20/20 [==============================] - 9s 436ms/step - loss: 0.4628 - fn_acc: 0.8171 - fn_loss: 0.9797 - val_loss: 0.4659 - val_fn_acc: 0.8151 - val_fn_loss: 0.9783\n",
      "Epoch 86/100\n",
      "20/20 [==============================] - 9s 436ms/step - loss: 0.4591 - fn_acc: 0.8158 - fn_loss: 0.9721 - val_loss: 0.4419 - val_fn_acc: 0.8232 - val_fn_loss: 0.9261\n",
      "Epoch 87/100\n",
      "20/20 [==============================] - 9s 445ms/step - loss: 0.4827 - fn_acc: 0.8113 - fn_loss: 0.9944 - val_loss: 0.3993 - val_fn_acc: 0.8276 - val_fn_loss: 0.8988\n",
      "Epoch 88/100\n",
      "20/20 [==============================] - 10s 499ms/step - loss: 0.4641 - fn_acc: 0.8146 - fn_loss: 0.9690 - val_loss: 0.4896 - val_fn_acc: 0.8085 - val_fn_loss: 1.0036\n",
      "Epoch 89/100\n",
      "20/20 [==============================] - 9s 455ms/step - loss: 0.5305 - fn_acc: 0.8032 - fn_loss: 1.0436 - val_loss: 0.5001 - val_fn_acc: 0.8063 - val_fn_loss: 1.0195\n",
      "Epoch 90/100\n",
      "20/20 [==============================] - 9s 468ms/step - loss: 0.5007 - fn_acc: 0.8089 - fn_loss: 0.9966 - val_loss: 0.4427 - val_fn_acc: 0.8187 - val_fn_loss: 0.9396\n",
      "Epoch 91/100\n",
      "20/20 [==============================] - 9s 457ms/step - loss: 0.4919 - fn_acc: 0.8076 - fn_loss: 1.0063 - val_loss: 0.5139 - val_fn_acc: 0.8013 - val_fn_loss: 1.0244\n",
      "Epoch 92/100\n",
      "20/20 [==============================] - 9s 464ms/step - loss: 0.5159 - fn_acc: 0.8030 - fn_loss: 1.0279 - val_loss: 0.4655 - val_fn_acc: 0.8220 - val_fn_loss: 0.9477\n",
      "Epoch 93/100\n",
      "20/20 [==============================] - 9s 430ms/step - loss: 0.4844 - fn_acc: 0.8154 - fn_loss: 0.9824 - val_loss: 0.4594 - val_fn_acc: 0.8172 - val_fn_loss: 0.9494\n",
      "Epoch 94/100\n",
      " 7/20 [=========>....................] - ETA: 3s - loss: 0.5156 - fn_acc: 0.8040 - fn_loss: 1.0042"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20/20 [==============================] - 20s 1s/step - loss: 1.0171 - fn_acc: 0.3073 - fn_loss: 1.0184 - val_loss: 0.8814 - val_fn_acc: 0.2821 - val_fn_loss: 0.8811\n",
      "Epoch 94/100\n",
      "20/20 [==============================] - 21s 1s/step - loss: 0.8888 - fn_acc: 0.2997 - fn_loss: 0.8885 - val_loss: 0.9486 - val_fn_acc: 0.3091 - val_fn_loss: 0.9476\n",
      "Epoch 95/100\n",
      "20/20 [==============================] - 20s 999ms/step - loss: 0.9234 - fn_acc: 0.2970 - fn_loss: 0.9231 - val_loss: 0.8901 - val_fn_acc: 0.2968 - val_fn_loss: 0.8878\n",
      "Epoch 96/100\n",
      "20/20 [==============================] - 20s 998ms/step - loss: 0.9167 - fn_acc: 0.3061 - fn_loss: 0.9165 - val_loss: 0.9429 - val_fn_acc: 0.3083 - val_fn_loss: 0.9426\n",
      "Epoch 97/100\n",
      "20/20 [==============================] - 21s 1s/step - loss: 0.9587 - fn_acc: 0.3051 - fn_loss: 0.9592 - val_loss: 0.8660 - val_fn_acc: 0.2779 - val_fn_loss: 0.8640\n",
      "Epoch 98/100\n",
      "20/20 [==============================] - 21s 1s/step - loss: 0.9148 - fn_acc: 0.2920 - fn_loss: 0.9133 - val_loss: 0.8890 - val_fn_acc: 0.3004 - val_fn_loss: 0.8890\n",
      "Epoch 99/100\n",
      "20/20 [==============================] - 21s 1s/step - loss: 0.9059 - fn_acc: 0.2968 - fn_loss: 0.9057 - val_loss: 0.9008 - val_fn_acc: 0.2882 - val_fn_loss: 0.9009\n",
      "Epoch 100/100\n",
      "20/20 [==============================] - 20s 1s/step - loss: 0.8990 - fn_acc: 0.2946 - fn_loss: 0.8982 - val_loss: 0.9168 - val_fn_acc: 0.2995 - val_fn_loss: 0.9160\n"
     ]
    }
   ],
   "source": [
    "# Train the model using the `fit` method.\n",
    "\n",
    "# Arguments:\n",
    "# train_ds.repeat(): Training dataset. The `repeat()` method repeats the dataset indefinitely.\n",
    "# epochs=100: Number of epochs (iterations over the entire dataset).\n",
    "# steps_per_epoch=20: Number of steps (batches) to yield from the training dataset in each epoch.\n",
    "# validation_data=test_ds: Validation dataset.\n",
    "# validation_steps=20: Number of steps (batches) to yield from the validation dataset after each epoch.\n",
    "\n",
    "history = model.fit(\n",
    "    train_ds.repeat(),\n",
    "    epochs=100,\n",
    "    steps_per_epoch = 20,\n",
    "    validation_data=test_ds,\n",
    "    validation_steps = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 466
    },
    "id": "r4W8ZeXoX1g5",
    "outputId": "372eba38-969e-4d98-9606-a084f0a66493"
   },
   "outputs": [],
   "source": [
    "plt.plot(history.history['fn_loss'], label='loss')\n",
    "plt.plot(history.history['val_fn_loss'], label='val_loss')\n",
    "plt.ylim([0, max(plt.ylim())])\n",
    "plt.xlabel('Epoch #')\n",
    "plt.ylabel('CE')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 466
    },
    "id": "LKfLOT_FX_eX",
    "outputId": "e537aa50-c2bc-4492-982c-426efda39622"
   },
   "outputs": [],
   "source": [
    "plt.plot(history.history['fn_acc'], label='accuracy')\n",
    "plt.plot(history.history['val_fn_acc'], label='val_accuracy')\n",
    "plt.ylim([0, max(plt.ylim())])\n",
    "plt.xlabel('Epoch #')\n",
    "plt.ylabel('CE')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OLSQHezFJZ49",
    "outputId": "df62ce49-bec8-4fac-8e4a-5c06deceb082"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "\n",
    "result = model.translate(['Ich habe die Geduld dafür'])\n",
    "predicted = result[0].numpy().decode()\n",
    "actual = \"I have the patience for this\"\n",
    "print(\"Predicted:\", predicted)\n",
    "print(\"Actual:\", actual)\n",
    "\n",
    "# Calculate BLEU score\n",
    "bleu_score = sentence_bleu([predicted], actual)\n",
    "\n",
    "print(\"BLEU Score for Reference:\", bleu_score)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
